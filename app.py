# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-3PKXlCDCxqEm-kv1TwRovbyt6nqJWcw
"""

import streamlit as st
import numpy as np
import faiss
import pickle
import os
from io import BytesIO
from pdfminer.high_level import extract_text
from sentence_transformers import SentenceTransformer
import re
from typing import List, Dict, Any
import logging
import google.generativeai as genai

# ==== CONFIG ====
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
INDEX_PATH = "financial_index/faiss_index.bin"
METADATA_PATH = "financial_index/metadata.pkl"
GEMINI_MODEL = "gemini-pro"

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==== INIT ====
st.set_page_config(
    page_title="ðŸ“Š Financial Report RAG Bot",
    page_icon="ðŸ“‰",
    layout="wide"
)

os.makedirs("financial_index", exist_ok=True)

# Configure Gemini with secret API key
genai.configure(api_key=st.secrets["GEMINI_API_KEY"])

@st.cache_resource
def load_model():
    """Load the sentence transformer model with caching."""
    return SentenceTransformer(EMBEDDING_MODEL)

@st.cache_resource
def load_index():
    """Load or create FAISS index with caching."""
    if os.path.exists(INDEX_PATH):
        return faiss.read_index(INDEX_PATH)
    else:
        return faiss.IndexFlatL2(384)

def load_metadata():
    """Load metadata with error handling."""
    try:
        if os.path.exists(METADATA_PATH):
            with open(METADATA_PATH, "rb") as f:
                return pickle.load(f)
    except Exception as e:
        logger.error(f"Error loading metadata: {e}")
    return []

# Initialize components
model = load_model()
index = load_index()
metadata = load_metadata()

# ==== ENHANCED UTILS ====
def clean_text(text: str) -> str:
    """Clean and preprocess text."""
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s.,;:!?()-]', '', text)
    return text.strip()

def create_smart_chunks(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """Create overlapping chunks with sentence boundary awareness."""
    sentences = text.split('. ')
    chunks = []
    current_chunk = ""

    for sentence in sentences:
        if len(current_chunk) + len(sentence) < chunk_size:
            current_chunk += sentence + ". "
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            overlap_text = '. '.join(current_chunk.split('. ')[-overlap//50:]) if overlap > 0 else ""
            current_chunk = overlap_text + sentence + ". "

    if current_chunk:
        chunks.append(current_chunk.strip())

    return [chunk for chunk in chunks if len(chunk.strip()) > 20]

def retrieve(query: str, top_k: int = 5, company_filter: str = None) -> List[Dict[str, Any]]:
    """Enhanced retrieval with optional company filtering."""
    try:
        q_vec = model.encode([query])
        D, I = index.search(np.array(q_vec).astype("float32"), min(top_k * 2, len(metadata)))

        results = []
        for i, distance in zip(I[0], D[0]):
            if i < len(metadata):
                chunk = metadata[i].copy()
                chunk['score'] = float(distance)

                if company_filter and company_filter.lower() not in chunk.get('company', '').lower():
                    continue

                results.append(chunk)

        results.sort(key=lambda x: x['score'])
        return results[:top_k]

    except Exception as e:
        logger.error(f"Error in retrieval: {e}")
        return []

def generate_with_gemini(prompt: str, max_tokens: int = 500) -> str:
    """Generate response using Google Gemini."""
    try:
        model = genai.GenerativeModel(GEMINI_MODEL)
        response = model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=max_tokens,
                temperature=0.7,
                top_p=0.9
            )
        )
        return response.text.strip() if response and response.text else "âŒ No valid response from Gemini."
    except Exception as e:
        return f"âŒ Gemini error: {e}"

def process_pdf(file, company: str = "unknown") -> int:
    """Enhanced PDF processing with better chunking."""
    try:
        text = extract_text(file)
        text = clean_text(text)

        if len(text.strip()) < 100:
            st.error("PDF appears to be empty or unreadable.")
            return 0

        chunks = create_smart_chunks(text)

        if not chunks:
            st.error("No valid text chunks extracted from PDF.")
            return 0

        batch_size = 32
        for i in range(0, len(chunks), batch_size):
            batch_chunks = chunks[i:i + batch_size]
            vectors = model.encode(batch_chunks)

            for j, chunk in enumerate(batch_chunks):
                index.add(np.array([vectors[j]]).astype("float32"))
                metadata.append({
                    "id": len(metadata),
                    "content": chunk,
                    "company": company,
                    "chunk_index": i + j
                })

        faiss.write_index(index, INDEX_PATH)
        with open(METADATA_PATH, "wb") as f:
            pickle.dump(metadata, f)

        return len(chunks)

    except Exception as e:
        logger.error(f"PDF processing error: {e}")
        st.error(f"Error processing PDF: {e}")
        return 0

# ==== SESSION STATE ====
if "history" not in st.session_state:
    st.session_state.history = []

# ==== SIDEBAR ====
st.sidebar.title("âš™ï¸ Financial Docs Management")

st.sidebar.subheader("ðŸ“‚ Upload Documents")
uploaded = st.sidebar.file_uploader("Upload Financial PDF", type="pdf", accept_multiple_files=True)
company_tag = st.sidebar.text_input("ðŸ·ï¸ Company Name", placeholder="e.g., Apple Inc.")

if uploaded:
    for file in uploaded:
        with st.spinner(f"ðŸ”„ Processing {file.name}..."):
            added = process_pdf(BytesIO(file.read()), company=company_tag or file.name)
            if added > 0:
                st.sidebar.success(f"âœ… Added {added} chunks from {file.name}")

st.sidebar.subheader("ðŸ—„ï¸ Index Management")
st.sidebar.write(f"ðŸ“Š Total chunks: {len(metadata)}")
companies = list(set([item.get('company', 'unknown') for item in metadata]))
if companies:
    st.sidebar.write(f"ðŸ¢ Companies: {', '.join(companies)}")

if st.sidebar.button("ðŸ§¹ Reset Index", type="secondary"):
    if os.path.exists(INDEX_PATH): os.remove(INDEX_PATH)
    if os.path.exists(METADATA_PATH): os.remove(METADATA_PATH)
    index.reset()
    metadata.clear()
    st.sidebar.success("âœ… Index cleared!")
    st.experimental_rerun()

st.sidebar.subheader("ðŸŽ›ï¸ Search Options")
company_filter = st.sidebar.selectbox(
    "Filter by Company",
    options=["All"] + sorted(companies),
    index=0
)
top_k = st.sidebar.slider("Results to retrieve", 1, 10, 5)

# ==== MAIN INTERFACE ====
st.title("ðŸ“Š Financial Report RAG Bot")
st.markdown("""
Upload financial reports (PDF) and ask questions.
The bot retrieves relevant sections and answers using **Google Gemini**.
""")

if len(metadata) == 0:
    st.warning("ðŸ‘ˆ Upload some PDF documents to get started!")
else:
    st.success(f"âœ… Ready to answer questions about {len(metadata)} document chunks!")

query = st.text_input("ðŸ’¬ Ask your question:", placeholder="e.g., What was the revenue growth in Q4?")

if query and len(metadata) > 0:
    with st.spinner("ðŸ” Searching and generating answer..."):
        filter_company = None if company_filter == "All" else company_filter
        chunks = retrieve(query, top_k=top_k, company_filter=filter_company)

        if not chunks:
            st.warning("No relevant information found. Try rephrasing your question.")
        else:
            context = "\n\n".join([f"[{chunk['company']}]: {chunk['content']}" for chunk in chunks])

            prompt = f"""You are a professional financial analyst assistant. Use the provided context from financial documents to answer the question accurately and comprehensively.

CONTEXT FROM FINANCIAL DOCUMENTS:
{context}

QUESTION: {query}

INSTRUCTIONS:
- Provide a clear, accurate answer based on the context
- Include specific numbers, percentages, or financial metrics when available
- If the context doesn't fully answer the question, state what information is missing
- Be concise but thorough
- Use professional financial terminology

ANSWER:"""

            answer = generate_with_gemini(prompt, max_tokens=300)
            st.session_state.history.append((query, answer, chunks))

# ==== DISPLAY RESULTS ====
if st.session_state.history:
    st.markdown("---")
    st.subheader("ðŸ’¬ Latest Response")

    latest_query, latest_answer, latest_chunks = st.session_state.history[-1]

    st.markdown(f"**Question:** {latest_query}")
    st.markdown(f"**Answer:** {latest_answer}")

    with st.expander("ðŸ“„ Sources Used"):
        for i, chunk in enumerate(latest_chunks, 1):
            st.markdown(f"""
            **Source {i}** - *{chunk.get('company', 'N/A')}* (Relevance Score: {chunk.get('score', 0):.3f})

            {chunk['content'][:300]}...
            """)

    if len(st.session_state.history) > 1:
        with st.expander(f"ðŸ“š Previous Questions ({len(st.session_state.history) - 1})"):
            for i, (q, a, _) in enumerate(reversed(st.session_state.history[:-1]), 1):
                st.markdown(f"**Q{len(st.session_state.history) - i}:** {q}")
                st.markdown(f"**A:** {a[:200]}...")
                st.markdown("---")

# ==== FOOTER ====
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: gray;'>
<small>Financial RAG Bot | Powered by FAISS, Sentence Transformers & Google Gemini</small>
</div>
""", unsafe_allow_html=True)